#!/bin/bash

# åœ¨GPU 1-7ä¸Šå¹¶è¡Œè¿è¡ŒæŽ¨ç†
# æ¯ä¸ªGPUå¤„ç†ä¸åŒçš„æ•°æ®åˆ†ç‰‡

echo "ðŸš€ Starting multi-GPU inference on GPUs 1-7..."

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p inference_results

# åœ¨åŽå°å¯åŠ¨7ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªä½¿ç”¨ä¸€ä¸ªGPU
for gpu_id in {1..7}; do
    echo "Starting inference on GPU $gpu_id..."
    CUDA_VISIBLE_DEVICES=$gpu_id python inference.py \
        --config yaml/infer.yaml \
        --model_checkpoint checkpoints/Qwen-0.5B \
        --output_dir inference_results/gpu_$gpu_id \
        --batch_size 4 \
        > logs/gpu_$gpu_id.log 2>&1 &
done

echo "âœ… All inference processes started. Check logs/ directory for progress."
echo "Use 'ps aux | grep inference.py' to check running processes"
echo "Use 'tail -f logs/gpu_*.log' to monitor progress" 