#!/usr/bin/env python
# -*- coding:utf-8 _*-
import importlib
import json
from transformers import Trainer, TrainingArguments
from transformers.trainer_utils import EvalPrediction
from transformers.trainer_callback import TrainerCallback
import os
import torch
from models.TimeLanguageModel import TLM, TLMConfig
from dataset.dataset import DataCollator
from typing import Dict, List, Any, NamedTuple, Optional, Tuple, Union
from datasets import load_metric
import numpy as np
from utils.metrics import open_question_metrics,closed_question_metrics,compute_rul
import warnings
from tqdm import tqdm
import pickle
from torch import nn
import pandas as pd
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")
from accelerate import Accelerator
accelerator = Accelerator()
from models.TimeLanguageModel import GradientAndActivationMonitor 
import torch.distributed as dist    
from datetime import datetime

def distributed_tqdm(iterable, desc=None):
    if not dist.is_initialized() or dist.get_rank() == 0:
        return tqdm(iterable, desc=desc)
    else:
        return iterable

class OutputWrapper:
    def __init__(self, original_output):
        self.original_output = original_output

    def __getattr__(self, name):
        # å¦‚æœå±æ€§ä¸å­˜åœ¨äºè‡ªèº«ï¼Œåˆ™å°è¯•ä»åŸå§‹å¯¹è±¡ä¸­è·å–
        return getattr(self.original_output, name)
    
class EvalLoopOutput(NamedTuple):
    predictions: Union[np.ndarray, Tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
    metrics: Optional[Dict[str, float]]
    num_samples: Optional[int]
    pred_extra: Optional[Dict[str, Any]] = None



class Exp_Instruct(Trainer):
    def __init__(self, args, train_dataset, tlm_config=None, eval_dataset=None):
        # Build the model
        self.tlmconfig = tlm_config
        model = self._build_model(args)
        # Define training arguments
        training_args = TrainingArguments(
            output_dir=args.output_dir,
            per_device_train_batch_size=args.per_device_train_batch_size,
            dataloader_num_workers = args.dataloader_num_workers,
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            per_device_eval_batch_size=args.per_device_eval_batch_size,
            learning_rate=args.learning_rate,
            weight_decay=args.weight_decay,
            logging_dir=args.output_dir,
            logging_steps=args.logging_steps,
            save_steps=args.save_steps,
            eval_strategy='no',
            eval_steps=args.eval_steps,
            save_total_limit=args.save_total_limit,
            ddp_find_unused_parameters=False,  
            fp16=args.fp16,  
            num_train_epochs=args.num_train_epochs,
            report_to=args.report_to,  # Example: Integrate TensorBoard
            prediction_loss_only=False,
            max_grad_norm=0.1,
            dataloader_drop_last=True)

        super().__init__(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=DataCollator(tokenizer=train_dataset.tokenizer),
            eval_dataset=eval_dataset,
            # compute_metrics=self._compute_metrics if eval_dataset else None,
        )
        self.compute_metrics  = self.custom_compute_metrics if eval_dataset else None
        self.special_id = train_dataset.processor.all_special_ids  
        self.processor = train_dataset.processor
        self.padding_idx = self.processor.pad_token_id
        # å¸¸ç”¨æ ‡ç‚¹ç¬¦å·åˆ—è¡¨
        common_punctuations = [".", ",", ":", ";", "!", "?", "(", ")", "[", "]", "{", "}", "-", "_", "\"", "'"]
        punctuation_ids = self.processor.convert_tokens_to_ids(common_punctuations)
        # å°†æ ‡ç‚¹ç¬¦å· ID åˆå¹¶åˆ°ç‰¹æ®Šæ ‡è®° ID åˆ—è¡¨ä¸­
        self.special_id.extend(punctuation_ids)
        self.tlmargs = args
        
        # å®šä¹‰stageæƒé‡
        self.stage_weights = {
            1: 1.0,   # å¼€æ”¾å¼é—®é¢˜ - åŸºç¡€æƒé‡
            2: 1.5,   # å°é—­å¼é—®é¢˜ - ç¨é«˜æƒé‡
            3: 1.5,   # å°é—­å¼é—®é¢˜ - ä¸­ç­‰æƒé‡
            4: 1.1    # å¼€æ”¾å¼é—®é¢˜ - ç¨ä½æƒé‡
        }
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°ï¼Œä¸ä½¿ç”¨ignore_indexï¼ˆæˆ‘ä»¬å°†æ‰‹åŠ¨å¤„ç†ï¼‰
        self.base_loss_fn = nn.CrossEntropyLoss(reduction='none', ignore_index=self.padding_idx)
        # self.args.remove_unused_columns = True  # æ·»åŠ è¿™ä¸€è¡Œ
    def load_model(self, checkpoint_path):
        self.model = TLM.from_pretrained(checkpoint_path, config=self.tlmconfig, ts_config=self.tlmargs).cuda()

    def _build_model(self, args):
        """Load the model dynamically based on the configuration."""
        # self.tlmconfig = TLMConfig(llm_model_path = args.llm_model_path)
        model = TLM(self.tlmconfig,args).cuda()
        if accelerator.is_main_process:
            # è¾“å‡ºæ¨¡å‹ç»“æ„
            print("æ¨¡å‹ç»“æ„ï¼š")
            print(model)

            # è®¡ç®—å¹¶è¾“å‡ºå¯è®­ç»ƒå‚æ•°é‡ï¼ˆä»¥ç™¾ä¸‡ä¸ºå•ä½ï¼‰
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            trainable_params_m = trainable_params / 1e6  # è½¬æ¢ä¸ºç™¾ä¸‡ï¼ˆMï¼‰

            print(f"æ¨¡å‹è®­ç»ƒå‚æ•°é‡ä¸ºï¼š{trainable_params_m:.2f}M")
        # monitor = GradientAndActivationMonitor(model,track_outputs=False,verbose=True)
        return model
    
    def concat_np_array(self, array_list,num_samples):
        """
        å¯¹ä¼ å…¥çš„åˆ—è¡¨è¿›è¡Œ Concat æ“ä½œã€‚
        
        Args:
            array_list (List[List[int]]): æ¯ä¸ªå­åˆ—è¡¨ä¸ºéœ€è¦ Padding çš„åºåˆ—ã€‚
            num_samples (int): æ ·æœ¬æ•°é‡ã€‚
        Returns:
            np.ndarray: Padding åçš„äºŒç»´æ•°ç»„ã€‚
        """
        # è·å–æœ€å¤§é•¿åº¦
        max_length = max(arr.shape[-1] for arr in array_list)
        
        # åˆå§‹åŒ– Padding åçš„æ•°ç»„ï¼Œå¡«å……ä¸º padding_idx
        padded_array = np.full((num_samples, max_length), self.padding_idx, dtype=np.int32)
        
        # å¡«å……æ¯ä¸ªåºåˆ—
        for i, arr in enumerate(array_list):
            padded_array[:arr.shape[0], :arr.shape[1]] = arr
        concat_array = np.stack(padded_array, axis=0)
        return concat_array    

    def debug_generate(self, input_ids, query_ids,ts_values, stage, attention_mask):
        # ç”Ÿæˆé˜¶æ®µ
        import time
        start_time = time.time()
        with torch.no_grad():
            output = self.model.generate(
                input_ids=input_ids,
                query_ids=query_ids,
                ts_values=ts_values,
                stage=stage,
                past_key_values=None,
                max_new_tokens=128,
                do_sample=False,
                eos_token_id=self.processor.eos_token_id,
                pad_token_id=self.processor.pad_token_id,
                attention_mask=attention_mask,
                use_cache=True,
                # æ–°å¢ä¼˜åŒ–å‚æ•°
                num_beams=1,                    # è´ªå©ªæœç´¢ï¼Œæœ€å¿«
                temperature=1.0,                # é¿å…é¢å¤–è®¡ç®—
                top_p=None,                     # å…³é—­nucleus sampling
                top_k=None,                     # å…³é—­top-k sampling
                repetition_penalty=1.0,         # å…³é—­é‡å¤æƒ©ç½š
                length_penalty=1.0,             # å…³é—­é•¿åº¦æƒ©ç½š
                no_repeat_ngram_size=0,         # å…³é—­n-gramé‡å¤æ£€æŸ¥
                output_scores=False,            # ä¸è¾“å‡ºåˆ†æ•°
                output_attentions=False,        # ä¸è¾“å‡ºattention
                output_hidden_states=False,     # ä¸è¾“å‡ºéšè—çŠ¶æ€
                return_dict_in_generate=False,  # ç®€åŒ–è¿”å›æ ¼å¼
            )
        end_time = time.time()
        if accelerator.is_main_process:
            # generated_ids = output[:, input_ids.shape[1]:]
            print(f"[ç”Ÿæˆè€—æ—¶] {end_time - start_time:.2f} ç§’")
            # è§£ç è¾“å‡ºä¸ºæ–‡å­—
            try:
                generated_text = self.processor.decode(output[0], skip_special_tokens=True)
                print(f"[ç”Ÿæˆç»“æœ] {generated_text}")
            except Exception as e:
                print(f"[é”™è¯¯] æ— æ³•è§£ç  generated_ids: {e}")
        return output
    def generate(
        self,
        dataloader,
        description,
        prediction_loss_only=None,
        ignore_keys=None,
        metric_key_prefix="eval",
    ):
        all_predictions = []
        all_labels = []
        all_losses = []
        all_index = []

        model = self._wrap_model(self.model, training=False)
        model.eval()
        sample_num = len(dataloader.dataset)
        # forms = []
        stages = []
        with torch.no_grad():
            for step, inputs in enumerate(distributed_tqdm(dataloader, desc=description)):
                # if step==50:
                #     break

                input_ids = inputs['input_ids']
                ts_values = inputs['ts_values']
                stage = inputs['stage']
                index = inputs['index']
                query_ids = inputs['query_ids']
                attention_mask =inputs['attention_mask']
                generated_ids = self.debug_generate(input_ids, 
                                                    query_ids,ts_values, stage, attention_mask)

                prediction = generated_ids.cpu().numpy()
                all_predictions.extend(prediction)
                all_labels.extend(inputs["labels"].cpu().numpy())

                # forms.extend(inputs['form'])
                stages.extend(inputs['stage'].tolist())

                all_index.extend(inputs['index'].tolist())

        filtered_preds, filtered_labels = [], []
        str_predictions = self.processor.batch_decode(all_predictions,skip_special_tokens=True)
        str_labels = self.processor.batch_decode(all_labels,skip_special_tokens=True)
        #å–å‡ºassistant\nåçš„å†…å®¹
        str_predictions = [pred.split('assistant\n')[-1] for pred in str_predictions]
        output_data = {
            "predictions": str_predictions,
            "labels": str_labels,
            "stages": stages,
            "index": all_index,
            "num_samples": sample_num
        }

        with open('output_result_all.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4, ensure_ascii=False)

        pred_extra = {'stages': stages}
        avg_loss = np.mean(all_losses) if all_losses else None
        return EvalLoopOutput(predictions=str_predictions, label_ids=str_labels,
                               metrics=avg_loss, num_samples=sample_num,pred_extra=pred_extra)


    #å†™ä¸€ä¸ªè¿‡æ»¤str_predictionså’Œstr_labelsçš„å‡½æ•°

    def evaluate(
        self,
        eval_dataset=None,
        ignore_keys=None,
        metric_key_prefix="eval",
    ):
        eval_dataset = eval_dataset or self.eval_dataset
        eval_dataloader = self.get_eval_dataloader(eval_dataset)
        output = self.generate(
            eval_dataloader, 'eval'
        )

        metrics = self.custom_compute_metrics(output)

        if accelerator.is_main_process:
            # æ‰“å°åˆ°æ§åˆ¶å°
            print(metrics)
                # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'metrics_eval_{timestamp}.txt'
            # åŒæ—¶å†™å…¥æ–‡ä»¶

            with open(filename, 'w', encoding='utf-8') as f:
                print(metrics, file=f)


    def custom_compute_metrics(self,eval_pred: EvalLoopOutput) -> Dict[str, Any]:
        """
        é’ˆå¯¹ stages ä¸º 1 æˆ– 2 çš„æ ·æœ¬ï¼Œè®¡ç®— BLEU å’Œ ROUGE æŒ‡æ ‡ã€‚
        Args:
            eval_pred (EvalPrediction): åŒ…å« predictions å’Œ labelsï¼Œä»¥åŠé™„åŠ ä¿¡æ¯ pred_extraã€‚
        
        Returns:
            Dict[str, Any]: BLEU å’Œ ROUGE æŒ‡æ ‡ç»“æœå­—å…¸ã€‚
        """
        # è§£æé¢„æµ‹å’Œæ ‡ç­¾
        labels =  eval_pred.label_ids
        stages = eval_pred.pred_extra['stages']        
        # è§£æé™„åŠ ä¿¡æ¯
        # ç­›é€‰ stages ä¸º 1 
        stage1_indices = [i for i, stage in enumerate(stages) if stage in [1]]
        if  len(stage1_indices) >=1:
            # æå–å¯¹åº”çš„é¢„æµ‹å’Œæ ‡ç­¾
            stage1_labels = [labels[i] for i in stage1_indices]
            stage1_metrics = open_question_metrics([eval_pred.predictions[i] for i in stage1_indices], 
                                                   stage1_labels,self.special_id)

        #ç­›é€‰å‡ºstageä¸º2çš„æ ·æœ¬
        stage2_indices = [i for i, stage in enumerate(stages) if stage in [2]]
        if len(stage2_indices) >=1:
            # æå–å¯¹åº”çš„é¢„æµ‹å’Œæ ‡ç­¾
            stage2_labels = [labels[i] for i in stage2_indices]
            stage2_predictions = [eval_pred.predictions[i] for i in stage2_indices] 
            stage2_metrics = closed_question_metrics( stage2_predictions,
                                                     stage2_labels,self.special_id)

        #ç­›é€‰å‡ºstageä¸º3çš„æ ·æœ¬
        stage3_indices = [i for i, stage in enumerate(stages) if stage in [3]]
        if  len(stage3_indices)>=1 :
            # æå–å¯¹åº”çš„é¢„æµ‹å’Œæ ‡ç­¾
            stage3_labels = [labels[i] for i in stage3_indices]
            stage3_predictions = [eval_pred.predictions[i] for i in stage3_indices]
            stage3_metrics = closed_question_metrics( stage3_predictions, 
                                                     stage3_labels,self.special_id)

        #ç­›é€‰å‡ºstageä¸º4çš„æ ·æœ¬
        stage4_indices = [i for i, stage in enumerate(stages) if stage in [4]]
        if len(stage4_indices) >=1:
            # æå–å¯¹åº”çš„é¢„æµ‹å’Œæ ‡ç­¾
            stage4_labels = [labels[i] for i in stage4_indices]
            stage4_metrics = open_question_metrics([eval_pred.predictions[i] for i in stage4_indices],
                                                    stage4_labels,self.special_id)
        
        #åˆå¹¶å­˜åœ¨çš„æŒ‡æ ‡
        metrics = {}
        if stage1_indices:
            metrics.update({f"stage1_{k}": v for k, v in stage1_metrics.items()})
        if stage2_indices:
            metrics.update({f"stage2_{k}": v for k, v in stage2_metrics.items()})
        if stage3_indices:
            metrics.update({f"stage3_{k}": v for k, v in stage3_metrics.items()})
        if stage4_indices:
            metrics.update({f"stage4_{k}": v for k, v in stage4_metrics.items()})


        return metrics
    
    def compute_stage_weighted_loss(self, logits, labels, stages, attention_mask=None):
        """
        ä¿®æ­£ç‰ˆæœ¬ - ä¸éœ€è¦shiftï¼Œå› ä¸ºDatasetå·²ç»å¤„ç†äº†
        """
        batch_size, seq_len, vocab_size = logits.shape
        
        # ğŸ”§ ä¸éœ€è¦shiftï¼Œç›´æ¥ä½¿ç”¨
        flat_logits = logits.view(-1, vocab_size)  # [batch_size * seq_len, vocab_size]
        flat_labels = labels.view(-1)              # [batch_size * seq_len]
        
        # è®¡ç®—åŸºç¡€æŸå¤±ï¼ˆpaddingä¼šè¢«è‡ªåŠ¨ignoreï¼‰
        token_losses = self.base_loss_fn(flat_logits, flat_labels)  # [batch_size * seq_len]
        token_losses = token_losses.view(batch_size, seq_len)       # [batch_size, seq_len]
        
        # åˆ›å»ºæœ‰æ•ˆtokenæ©ç 
        valid_mask = (labels != self.padding_idx).float()  # [batch_size, seq_len]
        
        # åº”ç”¨stageæƒé‡
        stage_weights = torch.tensor([self.stage_weights.get(stage.item(), 1.0) 
                                    for stage in stages], 
                                    device=logits.device, dtype=torch.float32)
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„åŠ æƒæŸå¤±
        sample_losses = []
        for i in range(batch_size):
            valid_tokens = valid_mask[i].sum()  # æœ‰æ•ˆtokenæ•°é‡
            if valid_tokens > 0:
                # ğŸ”§ åªå¯¹æœ‰æ•ˆtokenè®¡ç®—å¹³å‡æŸå¤±
                sample_loss = (token_losses[i] * valid_mask[i]).sum() / valid_tokens * stage_weights[i]
            else:
                sample_loss = torch.tensor(0.0, device=logits.device)
            sample_losses.append(sample_loss)
        
        return torch.stack(sample_losses).mean()

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬çš„æŸå¤±è®¡ç®—
        """
        # ä½¿ç”¨torch.no_grad()å‡å°‘ä¸å¿…è¦çš„æ¢¯åº¦è®¡ç®—
        with torch.cuda.amp.autocast(enabled=self.args.fp16):
            # å‰å‘ä¼ æ’­
            outputs = model(
                input_ids=inputs.get('input_ids'),
                query_ids=inputs.get('query_ids'),
                ts_values=inputs.get('ts_values'),
                stage=inputs.get('stage'),
                attention_mask=inputs.get('attention_mask'),
                labels=inputs.get('labels')
            )
        
        # è·å–logits
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
        
        # è®¡ç®—æŸå¤±
        loss = self.compute_stage_weighted_loss(
            logits=logits,
            labels=inputs.get('labels'),
            stages=inputs.get('stage'),
            attention_mask=inputs.get('attention_mask')
        )
        
        if return_outputs:
            # æ¸…ç†ä¸å¿…è¦çš„è¾“å‡ºä»¥èŠ‚çœå†…å­˜
            if hasattr(outputs, 'past_key_values'):
                outputs.past_key_values = None
            if hasattr(outputs, 'hidden_states'):
                outputs.hidden_states = None
            if hasattr(outputs, 'attentions'):
                outputs.attentions = None
            
            wrapped_outputs = OutputWrapper(outputs)
            wrapped_outputs.loss = loss
            return loss, wrapped_outputs
        
        return loss
    
    def get_stage_loss_statistics(self, dataloader, num_samples=100):
        """
        åˆ†æä¸åŒstageçš„æŸå¤±åˆ†å¸ƒï¼Œç”¨äºè°ƒæ•´æƒé‡
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            num_samples: åˆ†æçš„æ ·æœ¬æ•°é‡
        
        Returns:
            Dict: åŒ…å«å„stageæŸå¤±ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        self.model.eval()
        stage_losses = {1: [], 2: [], 3: [], 4: []}
        
        with torch.no_grad():
            for i, inputs in enumerate(dataloader):
                if i >= num_samples:
                    break
                
                # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                for key in inputs:
                    if isinstance(inputs[key], torch.Tensor):
                        inputs[key] = inputs[key].to(self.model.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(**inputs)
                logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±
                labels = inputs['labels']
                stages = inputs['stage']
                attention_mask = inputs.get('attention_mask')
                
                batch_size, seq_len, vocab_size = logits.shape
                flat_logits = logits.view(-1, vocab_size)
                flat_labels = labels.view(-1)
                
                token_losses = self.base_loss_fn(flat_logits, flat_labels)
                token_losses = token_losses.view(batch_size, seq_len)
                
                if attention_mask is not None:
                    valid_mask = attention_mask.bool()
                else:
                    valid_mask = (labels != self.padding_idx)
                
                masked_losses = token_losses * valid_mask.float()
                valid_token_counts = valid_mask.sum(dim=1).float()
                valid_token_counts = torch.clamp(valid_token_counts, min=1.0)
                sample_losses = masked_losses.sum(dim=1) / valid_token_counts
                
                # æŒ‰stageæ”¶é›†æŸå¤±
                for j, stage in enumerate(stages):
                    stage_val = stage.item()
                    if stage_val in stage_losses:
                        stage_losses[stage_val].append(sample_losses[j].item())
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        statistics = {}
        for stage, losses in stage_losses.items():
            if losses:
                statistics[f'stage_{stage}'] = {
                    'mean': np.mean(losses),
                    'std': np.std(losses),
                    'count': len(losses),
                    'min': np.min(losses),
                    'max': np.max(losses)
                }
        
        return statistics

